# Cleanup and Validation Procedures for Coolify CLI Testing

**Version:** 1.0
**Date:** 2025-10-16
**Purpose:** Comprehensive cleanup procedures and validation checks for testing processes

## Executive Summary

This document outlines comprehensive cleanup procedures and validation checks for the Coolify CLI testing process. These procedures ensure zero impact on production systems, complete cleanup of test artifacts, and maintain environment integrity.

## 1. Artifact Cleanup Scripts

### 1.1 Test Resource Cleanup Script
```bash
#!/bin/bash
# cleanup-test-resources.sh - Remove all test-created resources

set -euo pipefail

# Configuration
COOLIFY_SERVER="${COOLIFY_SERVER:-https://coolify.example.com}"
TEST_PREFIX="${TEST_PREFIX:-test-}"
DRY_RUN="${DRY_RUN:-false}"
BACKUP_BEFORE_DELETE="${BACKUP_BEFORE_DELETE:-true}"
LOG_FILE="cleanup-$(date +%Y%m%d-%H%M%S).log"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Logging function
log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

# Function to display colored output
print_status() {
    local color=$1
    local message=$2
    echo -e "${color}${message}${NC}"
}

# Function to check if resource is test resource
is_test_resource() {
    local resource_name="$1"
    [[ "$resource_name" == "$TEST_PREFIX"* ]]
}

# Function to backup resource before deletion
backup_resource() {
    local resource_type="$1"
    local resource_id="$2"
    local resource_name="$3"

    if [ "$BACKUP_BEFORE_DELETE" = "true" ]; then
        local backup_dir="backups/$(date +%Y%m%d-%H%M%S)"
        mkdir -p "$backup_dir"

        print_status "$BLUE" "Creating backup of $resource_type '$resource_name'..."

        # Backup logic would go here - export configuration, data, etc.
        # For now, create a placeholder backup file
        echo "Backup of $resource_type '$resource_name' (ID: $resource_id)" > "$backup_dir/${resource_type}_${resource_name}.backup"

        print_status "$GREEN" "‚úì Backup created in $backup_dir"
    fi
}

# Function to delete test projects
cleanup_test_projects() {
    log "Cleaning up test projects..."

    # Get list of test projects
    local test_projects=()

    # This would use Coolify API to get projects
    # For now, simulate with placeholder data
    if [ "$DRY_RUN" = "true" ]; then
        print_status "$YELLOW" "DRY RUN: Would query for test projects starting with '$TEST_PREFIX'"
        test_projects=("test-webapp" "test-api" "test-database")
    else
        # Real implementation would use Coolify API
        # test_projects=$(coolify api list-projects | jq -r ".[] | select(.name | startswith(\"$TEST_PREFIX\")) | .id")
        test_projects=("test-webapp" "test-api" "test-database")
    fi

    for project in "${test_projects[@]}"; do
        if is_test_resource "$project"; then
            print_status "$BLUE" "Processing test project: $project"

            if [ "$DRY_RUN" = "true" ]; then
                print_status "$YELLOW" "DRY RUN: Would delete project '$project'"
            else
                backup_resource "project" "$project" "$project"

                # Real deletion would use Coolify API
                # coolify api delete-project "$project"
                print_status "$GREEN" "‚úì Deleted test project: $project"
                log "Deleted test project: $project"
            fi
        fi
    done
}

# Function to delete test applications
cleanup_test_applications() {
    log "Cleaning up test applications..."

    local test_apps=()

    if [ "$DRY_RUN" = "true" ]; then
        print_status "$YELLOW" "DRY RUN: Would query for test applications"
        test_apps=("test-app-v1" "test-app-v2" "test-staging")
    else
        # Real implementation
        test_apps=("test-app-v1" "test-app-v2" "test-staging")
    fi

    for app in "${test_apps[@]}"; do
        if is_test_resource "$app"; then
            print_status "$BLUE" "Processing test application: $app"

            if [ "$DRY_RUN" = "true" ]; then
                print_status "$YELLOW" "DRY RUN: Would delete application '$app'"
            else
                backup_resource "application" "$app" "$app"

                # Stop application first
                print_status "$BLUE" "Stopping application..."
                # coolify api stop-application "$app"

                # Delete application
                print_status "$BLUE" "Deleting application..."
                # coolify api delete-application "$app"

                print_status "$GREEN" "‚úì Deleted test application: $app"
                log "Deleted test application: $app"
            fi
        fi
    done
}

# Function to delete test databases
cleanup_test_databases() {
    log "Cleaning up test databases..."

    local test_databases=()

    if [ "$DRY_RUN" = "true" ]; then
        print_status "$YELLOW" "DRY RUN: Would query for test databases"
        test_databases=("test_db_main" "test_db_cache" "test_db_logs")
    else
        # Real implementation
        test_databases=("test_db_main" "test_db_cache" "test_db_logs")
    fi

    for db in "${test_databases[@]}"; do
        if is_test_resource "$db"; then
            print_status "$BLUE" "Processing test database: $db"

            if [ "$DRY_RUN" = "true" ]; then
                print_status "$YELLOW" "DRY RUN: Would delete database '$db'"
            else
                backup_resource "database" "$db" "$db"

                # Delete database
                # coolify api delete-database "$db"

                print_status "$GREEN" "‚úì Deleted test database: $db"
                log "Deleted test database: $db"
            fi
        fi
    done
}

# Function to delete test domains
cleanup_test_domains() {
    log "Cleaning up test domains..."

    local test_domains=()

    if [ "$DRY_RUN" = "true" ]; then
        print_status "$YELLOW" "DRY RUN: Would query for test domains"
        test_domains=("test.example.com" "test-staging.example.com")
    else
        # Real implementation
        test_domains=("test.example.com" "test-staging.example.com")
    fi

    for domain in "${test_domains[@]}"; do
        if [[ "$domain" == test* ]]; then
            print_status "$BLUE" "Processing test domain: $domain"

            if [ "$DRY_RUN" = "true" ]; then
                print_status "$YELLOW" "DRY RUN: Would delete domain '$domain'"
            else
                backup_resource "domain" "$domain" "$domain"

                # Delete domain
                # coolify api delete-domain "$domain"

                print_status "$GREEN" "‚úì Deleted test domain: $domain"
                log "Deleted test domain: $domain"
            fi
        fi
    done
}

# Function to cleanup local test files
cleanup_local_files() {
    log "Cleaning up local test files..."

    # Remove test configuration files
    local test_files=(
        "test-config.yml"
        "test-deployment.json"
        ".coolify-test/"
        "test-results/"
        "test-backups/"
        "coolify-test-*.log"
        "test-*.tmp"
    )

    for file_pattern in "${test_files[@]}"; do
        if ls $file_pattern 1> /dev/null 2>&1; then
            if [ "$DRY_RUN" = "true" ]; then
                print_status "$YELLOW" "DRY RUN: Would remove files matching: $file_pattern"
            else
                rm -rf $file_pattern
                print_status "$GREEN" "‚úì Removed: $file_pattern"
                log "Removed local files: $file_pattern"
            fi
        fi
    done

    # Clean up test containers if any
    if command -v docker &> /dev/null; then
        local test_containers=$(docker ps -a --filter "name=test-" --format "{{.Names}}" 2>/dev/null || true)
        if [ -n "$test_containers" ]; then
            print_status "$BLUE" "Found test Docker containers, cleaning up..."

            if [ "$DRY_RUN" = "true" ]; then
                print_status "$YELLOW" "DRY RUN: Would remove test Docker containers"
            else
                echo "$test_containers" | xargs -r docker rm -f
                print_status "$GREEN" "‚úì Removed test Docker containers"
                log "Removed test Docker containers"
            fi
        fi
    fi
}

# Function to cleanup test network resources
cleanup_network_resources() {
    log "Cleaning up network resources..."

    # Clean up test DNS records (if managed)
    local test_dns_records=()

    if [ "$DRY_RUN" = "true" ]; then
        print_status "$YELLOW" "DRY RUN: Would clean up test DNS records"
    else
        # Implementation would depend on DNS provider
        print_status "$BLUE" "DNS record cleanup would be implemented here"
    fi

    # Clean up test SSL certificates
    local test_ssl_certs=()

    if [ "$DRY_RUN" = "true" ]; then
        print_status "$YELLOW" "DRY RUN: Would revoke test SSL certificates"
    else
        # Implementation would use certificate provider API
        print_status "$BLUE" "SSL certificate cleanup would be implemented here"
    fi
}

# Function to validate cleanup
validate_cleanup() {
    log "Validating cleanup completeness..."

    local validation_passed=true

    # Check for remaining test resources
    print_status "$BLUE" "Checking for remaining test resources..."

    # This would use Coolify API to check for remaining test resources
    if [ "$DRY_RUN" = "true" ]; then
        print_status "$YELLOW" "DRY RUN: Would validate no test resources remain"
    else
        # Real validation implementation
        local remaining_resources=$(coolify api list-resources 2>/dev/null | jq -r ".[] | select(.name | startswith(\"$TEST_PREFIX\")) | .name" || echo "")

        if [ -n "$remaining_resources" ]; then
            print_status "$RED" "‚ùå Found remaining test resources:"
            echo "$remaining_resources"
            validation_passed=false
        else
            print_status "$GREEN" "‚úì No test resources found"
        fi
    fi

    # Check for remaining local test files
    print_status "$BLUE" "Checking for remaining local test files..."
    local remaining_files=$(find . -name "test-*" -o -name ".coolify-test" 2>/dev/null || true)

    if [ -n "$remaining_files" ]; then
        print_status "$YELLOW" "‚ö†Ô∏è  Found remaining local test files:"
        echo "$remaining_files"
        print_status "$BLUE" "These may be intentional - review manually"
    else
        print_status "$GREEN" "‚úì No local test files found"
    fi

    if $validation_passed; then
        print_status "$GREEN" "üéâ Cleanup validation passed!"
        log "Cleanup validation passed"
        return 0
    else
        print_status "$RED" "‚ùå Cleanup validation failed!"
        log "Cleanup validation failed"
        return 1
    fi
}

# Function to generate cleanup report
generate_cleanup_report() {
    log "Generating cleanup report..."

    local report_file="cleanup-report-$(date +%Y%m%d-%H%M%S).json"

    cat > "$report_file" << EOF
{
  "cleanup_summary": {
    "timestamp": "$(date -Iseconds)",
    "coolify_server": "$COOLIFY_SERVER",
    "test_prefix": "$TEST_PREFIX",
    "dry_run": $DRY_RUN,
    "backup_enabled": $BACKUP_BEFORE_DELETE,
    "log_file": "$LOG_FILE"
  },
  "actions_performed": [
    "Test project cleanup",
    "Test application cleanup",
    "Test database cleanup",
    "Test domain cleanup",
    "Local file cleanup",
    "Network resource cleanup",
    "Cleanup validation"
  ],
  "backup_location": "$(if [ "$BACKUP_BEFORE_DELETE" = "true" ]; then echo "backups/$(date +%Y%m%d-%H%M%S)"; else echo "disabled"; fi)",
  "validation_result": "completed"
}
EOF

    print_status "$GREEN" "‚úì Cleanup report generated: $report_file"
    log "Cleanup report generated: $report_file"
}

# Main cleanup function
main() {
    print_status "$BLUE" "üßπ Starting Coolify test cleanup process..."
    print_status "$BLUE" "Server: $COOLIFY_SERVER"
    print_status "$BLUE" "Test Prefix: $TEST_PREFIX"
    print_status "$BLUE" "Dry Run: $DRY_RUN"
    print_status "$BLUE" "Backup: $BACKUP_BEFORE_DELETE"
    print_status "$BLUE" "Log File: $LOG_FILE"
    echo ""

    if [ "$DRY_RUN" = "true" ]; then
        print_status "$YELLOW" "‚ö†Ô∏è  DRY RUN MODE - No actual changes will be made"
        echo ""
    fi

    # Create backups directory if needed
    if [ "$BACKUP_BEFORE_DELETE" = "true" ] && [ "$DRY_RUN" = "false" ]; then
        mkdir -p backups
    fi

    # Execute cleanup steps
    cleanup_test_projects
    cleanup_test_applications
    cleanup_test_databases
    cleanup_test_domains
    cleanup_local_files
    cleanup_network_resources

    echo ""
    print_status "$BLUE" "üîç Validating cleanup completeness..."
    if validate_cleanup; then
        generate_cleanup_report
        print_status "$GREEN" "üéâ Cleanup process completed successfully!"
        log "Cleanup process completed successfully"
    else
        print_status "$RED" "‚ùå Cleanup process completed with validation errors"
        log "Cleanup process completed with validation errors"
        exit 1
    fi
}

# Parse command line arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        --dry-run)
            DRY_RUN="true"
            shift
            ;;
        --no-backup)
            BACKUP_BEFORE_DELETE="false"
            shift
            ;;
        --test-prefix)
            TEST_PREFIX="$2"
            shift 2
            ;;
        --server)
            COOLIFY_SERVER="$2"
            shift 2
            ;;
        --help)
            echo "Usage: $0 [OPTIONS]"
            echo "Options:"
            echo "  --dry-run        Show what would be deleted without actually deleting"
            echo "  --no-backup      Skip creating backups before deletion"
            echo "  --test-prefix    Prefix for test resources (default: test-)"
            echo "  --server         Coolify server URL"
            echo "  --help           Show this help message"
            exit 0
            ;;
        *)
            print_status "$RED" "Unknown option: $1"
            exit 1
            ;;
    esac
done

# Execute main function
main
```

### 1.2 Environment Reset Script
```python
#!/usr/bin/env python3
# environment-reset.py - Complete environment reset procedures

import os
import json
import subprocess
import shutil
from datetime import datetime
from pathlib import Path

class EnvironmentReset:
    def __init__(self, config_file="reset-config.json"):
        self.config = self.load_config(config_file)
        self.backup_dir = None
        self.log_file = f"reset-{datetime.now().strftime('%Y%m%d-%H%M%S')}.log"

    def load_config(self, config_file):
        """Load reset configuration"""
        default_config = {
            "backup_before_reset": True,
            "preserve_data": {
                "databases": [],
                "volumes": [],
                "config_files": []
            },
            "cleanup_paths": [
                "/tmp/coolify-test-*",
                "/var/log/coolify-test-*",
                ".coolify-test",
                "test-results",
                "test-artifacts"
            ],
            "services_to_stop": [
                "test-app-service",
                "test-db-service"
            ],
            "docker_cleanup": True,
            "network_cleanup": True,
            "dns_cleanup": True
        }

        try:
            with open(config_file, 'r') as f:
                config = json.load(f)
                # Merge with defaults
                return {**default_config, **config}
        except FileNotFoundError:
            return default_config

    def log(self, message):
        """Log message to file and console"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_entry = f"[{timestamp}] {message}"
        print(log_entry)

        with open(self.log_file, 'a') as f:
            f.write(log_entry + '\n')

    def create_backup(self):
        """Create backup before reset"""
        if not self.config['backup_before_reset']:
            self.log("Backup disabled")
            return True

        self.log("Creating backup before environment reset...")

        self.backup_dir = f"backup-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
        os.makedirs(self.backup_dir, exist_ok=True)

        # Backup configuration files
        config_files = [
            ".coolify/config.yml",
            "docker-compose.yml",
            "environment.yml"
        ]

        for config_file in config_files:
            if os.path.exists(config_file):
                backup_path = os.path.join(self.backup_dir, config_file)
                os.makedirs(os.path.dirname(backup_path), exist_ok=True)
                shutil.copy2(config_file, backup_path)
                self.log(f"Backed up: {config_file}")

        # Backup Docker data if requested
        if self.config.get('docker_backup', True):
            try:
                result = subprocess.run(
                    ['docker', 'ps', '-a', '--format', '{{.Names}}'],
                    capture_output=True, text=True
                )
                if result.returncode == 0:
                    containers = result.stdout.strip().split('\n')
                    test_containers = [c for c in containers if 'test' in c]

                    if test_containers:
                        backup_docker_dir = os.path.join(self.backup_dir, 'docker')
                        os.makedirs(backup_docker_dir, exist_ok=True)

                        # Export container configurations
                        for container in test_containers:
                            try:
                                result = subprocess.run(
                                    ['docker', 'inspect', container],
                                    capture_output=True, text=True
                                )
                                if result.returncode == 0:
                                    with open(f"{backup_docker_dir}/{container}.json", 'w') as f:
                                        f.write(result.stdout)
                                    self.log(f"Backed up Docker container: {container}")
                            except Exception as e:
                                self.log(f"Failed to backup container {container}: {e}")
            except Exception as e:
                self.log(f"Docker backup failed: {e}")

        self.log(f"Backup created in: {self.backup_dir}")
        return True

    def stop_services(self):
        """Stop test services"""
        self.log("Stopping test services...")

        services = self.config.get('services_to_stop', [])

        for service in services:
            try:
                # Try systemd service first
                result = subprocess.run(
                    ['systemctl', 'stop', service],
                    capture_output=True, text=True
                )
                if result.returncode == 0:
                    self.log(f"Stopped systemd service: {service}")
                    continue

                # Try Docker service
                result = subprocess.run(
                    ['docker', 'stop', service],
                    capture_output=True, text=True
                )
                if result.returncode == 0:
                    self.log(f"Stopped Docker container: {service}")
                    continue

                self.log(f"Service not found or already stopped: {service}")

            except Exception as e:
                self.log(f"Failed to stop service {service}: {e}")

    def cleanup_docker_resources(self):
        """Clean up Docker resources"""
        if not self.config.get('docker_cleanup', True):
            return

        self.log("Cleaning up Docker resources...")

        try:
            # Remove test containers
            result = subprocess.run(
                ['docker', 'ps', '-a', '--filter', 'name=test', '--format', '{{.Names}}'],
                capture_output=True, text=True
            )

            if result.returncode == 0:
                containers = result.stdout.strip().split('\n')
                containers = [c for c in containers if c]  # Remove empty strings

                for container in containers:
                    try:
                        subprocess.run(['docker', 'rm', '-f', container], check=True)
                        self.log(f"Removed container: {container}")
                    except subprocess.CalledProcessError as e:
                        self.log(f"Failed to remove container {container}: {e}")

            # Remove test images
            result = subprocess.run(
                ['docker', 'images', '--filter', 'reference=*/test-*', '--format', '{{.Repository}}:{{.Tag}}'],
                capture_output=True, text=True
            )

            if result.returncode == 0:
                images = result.stdout.strip().split('\n')
                images = [img for img in images if img]  # Remove empty strings

                for image in images:
                    try:
                        subprocess.run(['docker', 'rmi', '-f', image], check=True)
                        self.log(f"Removed image: {image}")
                    except subprocess.CalledProcessError as e:
                        self.log(f"Failed to remove image {image}: {e}")

            # Remove test volumes (excluding preserved ones)
            preserved_volumes = self.config.get('preserve_data', {}).get('volumes', [])

            result = subprocess.run(
                ['docker', 'volume', 'ls', '--filter', 'name=test', '--format', '{{.Name}}'],
                capture_output=True, text=True
            )

            if result.returncode == 0:
                volumes = result.stdout.strip().split('\n')
                volumes = [v for v in volumes if v]  # Remove empty strings

                for volume in volumes:
                    if volume not in preserved_volumes:
                        try:
                            subprocess.run(['docker', 'volume', 'rm', '-f', volume], check=True)
                            self.log(f"Removed volume: {volume}")
                        except subprocess.CalledProcessError as e:
                            self.log(f"Failed to remove volume {volume}: {e}")
                    else:
                        self.log(f"Preserved volume: {volume}")

            # Clean up unused Docker resources
            subprocess.run(['docker', 'system', 'prune', '-f'], check=True)
            self.log("Cleaned up unused Docker resources")

        except Exception as e:
            self.log(f"Docker cleanup failed: {e}")

    def cleanup_filesystem(self):
        """Clean up filesystem"""
        self.log("Cleaning up filesystem...")

        cleanup_paths = self.config.get('cleanup_paths', [])

        for path_pattern in cleanup_paths:
            try:
                # Use find to locate matching files/directories
                result = subprocess.run(
                    ['find', '.', '-name', path_pattern.replace('/tmp/', ''), '-type', 'f,d'],
                    capture_output=True, text=True
                )

                if result.returncode == 0:
                    paths = result.stdout.strip().split('\n')
                    paths = [p for p in paths if p]  # Remove empty strings

                    for path in paths:
                        try:
                            if os.path.isfile(path):
                                os.remove(path)
                                self.log(f"Removed file: {path}")
                            elif os.path.isdir(path):
                                shutil.rmtree(path)
                                self.log(f"Removed directory: {path}")
                        except Exception as e:
                            self.log(f"Failed to remove {path}: {e}")

            except Exception as e:
                self.log(f"Filesystem cleanup failed for pattern {path_pattern}: {e}")

        # Clean up temporary files
        temp_patterns = [
            '/tmp/coolify-test-*',
            '/tmp/coolify.*.tmp',
            '/tmp/test-coolify-*'
        ]

        for pattern in temp_patterns:
            try:
                result = subprocess.run(['find', '/tmp', '-name', pattern.replace('/tmp/', ''), '-delete'])
                if result.returncode == 0:
                    self.log(f"Cleaned temp files: {pattern}")
            except Exception as e:
                self.log(f"Temp file cleanup failed for {pattern}: {e}")

    def cleanup_network_resources(self):
        """Clean up network resources"""
        if not self.config.get('network_cleanup', True):
            return

        self.log("Cleaning up network resources...")

        # Clean up test network interfaces
        try:
            result = subprocess.run(
                ['ip', 'addr', 'show'],
                capture_output=True, text=True
            )

            # This would need custom logic based on your network setup
            self.log("Network cleanup would be implemented here")

        except Exception as e:
            self.log(f"Network cleanup failed: {e}")

    def cleanup_dns_records(self):
        """Clean up DNS records"""
        if not self.config.get('dns_cleanup', True):
            return

        self.log("Cleaning up DNS records...")

        # This would implement DNS record cleanup
        # Implementation depends on DNS provider (Cloudflare, AWS Route53, etc.)
        self.log("DNS cleanup would be implemented here")

    def reset_permissions(self):
        """Reset file permissions"""
        self.log("Resetting file permissions...")

        try:
            # Reset ownership of test-created files
            subprocess.run(['find', '.', '-name', 'test-*', '-user', 'root', '-exec', 'chown', '$USER:$USER', '{}', ';'])
            self.log("Reset file permissions")
        except Exception as e:
            self.log(f"Permission reset failed: {e}")

    def validate_reset(self):
        """Validate that reset was successful"""
        self.log("Validating environment reset...")

        validation_passed = True

        # Check for remaining test containers
        try:
            result = subprocess.run(
                ['docker', 'ps', '-a', '--filter', 'name=test', '--format', '{{.Names}}'],
                capture_output=True, text=True
            )

            if result.returncode == 0 and result.stdout.strip():
                remaining_containers = result.stdout.strip().split('\n')
                self.log(f"‚ö†Ô∏è  Found remaining test containers: {remaining_containers}")
                validation_passed = False
            else:
                self.log("‚úì No test containers found")
        except Exception as e:
            self.log(f"Container validation failed: {e}")

        # Check for remaining test files
        try:
            result = subprocess.run(
                ['find', '.', '-name', 'test-*', '-type', 'f'],
                capture_output=True, text=True
            )

            if result.returncode == 0 and result.stdout.strip():
                remaining_files = result.stdout.strip().split('\n')
                self.log(f"‚ö†Ô∏è  Found remaining test files: {remaining_files}")
            else:
                self.log("‚úì No test files found")
        except Exception as e:
            self.log(f"File validation failed: {e}")

        # Check system services
        services = self.config.get('services_to_stop', [])
        for service in services:
            try:
                result = subprocess.run(
                    ['systemctl', 'is-active', service],
                    capture_output=True, text=True
                )
                if result.returncode == 0 and result.stdout.strip() == 'active':
                    self.log(f"‚ö†Ô∏è  Service still active: {service}")
                    validation_passed = False
                else:
                    self.log(f"‚úì Service stopped: {service}")
            except:
                self.log(f"‚úì Service not found or stopped: {service}")

        return validation_passed

    def generate_reset_report(self):
        """Generate reset report"""
        self.log("Generating reset report...")

        report = {
            "reset_summary": {
                "timestamp": datetime.now().isoformat(),
                "backup_directory": self.backup_dir,
                "log_file": self.log_file,
                "config": self.config
            },
            "actions_performed": [
                "Environment backup creation",
                "Service stopping",
                "Docker resource cleanup",
                "Filesystem cleanup",
                "Network resource cleanup",
                "DNS record cleanup",
                "Permission reset",
                "Reset validation"
            ],
            "validation_passed": self.validate_reset()
        }

        report_file = f"reset-report-{datetime.now().strftime('%Y%m%d-%H%M%S')}.json"

        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2, default=str)

        self.log(f"Reset report generated: {report_file}")
        return report_file

    def reset_environment(self):
        """Perform complete environment reset"""
        self.log("üîÑ Starting environment reset process...")

        # Create backup
        if not self.create_backup():
            self.log("‚ùå Backup creation failed")
            return False

        # Stop services
        self.stop_services()

        # Clean up resources
        self.cleanup_docker_resources()
        self.cleanup_filesystem()
        self.cleanup_network_resources()
        self.cleanup_dns_records()

        # Reset permissions
        self.reset_permissions()

        # Validate reset
        if self.validate_reset():
            self.generate_reset_report()
            self.log("üéâ Environment reset completed successfully!")
            return True
        else:
            self.log("‚ùå Environment reset completed with validation errors")
            return False

def main():
    import argparse

    parser = argparse.ArgumentParser(description='Environment reset procedures')
    parser.add_argument('--config', default='reset-config.json', help='Configuration file')
    parser.add_argument('--no-backup', action='store_true', help='Skip backup creation')
    parser.add_argument('--dry-run', action='store_true', help='Show what would be done without executing')

    args = parser.parse_args()

    reset = EnvironmentReset(args.config)

    if args.no_backup:
        reset.config['backup_before_reset'] = False

    if args.dry_run:
        print("DRY RUN MODE - No actual changes will be made")
        print("Configuration:", json.dumps(reset.config, indent=2))
        return

    success = reset.reset_environment()
    exit(0 if success else 1)

if __name__ == "__main__":
    main()
```

## 2. Validation Checks

### 2.1 Test Remnants Validation
```python
#!/usr/bin/env python3
# validate-test-remnants.py - Check for test remnants

import os
import subprocess
import json
from pathlib import Path

class TestRemnantsValidator:
    def __init__(self):
        self.findings = []

    def log_finding(self, category, item, severity="warning"):
        """Log a finding"""
        finding = {
            "category": category,
            "item": item,
            "severity": severity,
            "timestamp": subprocess.run(['date', '-Iseconds'], capture_output=True, text=True).stdout.strip()
        }
        self.findings.append(finding)

        status_icon = "üî¥" if severity == "critical" else "üü°" if severity == "warning" else "üü¢"
        print(f"{status_icon} {category}: {item}")

    def check_filesystem_remnants(self):
        """Check for test files and directories"""
        print("üîç Checking filesystem for test remnants...")

        test_patterns = [
            "test-*",
            "*-test*",
            ".coolify-test",
            "test-artifacts",
            "test-results",
            "test-backups"
        ]

        for pattern in test_patterns:
            try:
                result = subprocess.run(
                    ['find', '.', '-name', pattern, '-type', 'f,d'],
                    capture_output=True, text=True
                )

                if result.returncode == 0:
                    items = result.stdout.strip().split('\n')
                    items = [item for item in items if item and item != '.']

                    for item in items:
                        # Check if it's actually a test remnant (not legitimate file)
                        if self.is_test_remnant(item):
                            self.log_finding("Filesystem", item, "warning")

            except Exception as e:
                print(f"Error checking pattern {pattern}: {e}")

    def is_test_remnant(self, path):
        """Determine if a path is a test remnant"""
        # Check common test indicators
        test_indicators = [
            "test-results",
            "test-artifacts",
            "test-backups",
            ".coolify-test",
            "coolify-test-",
            "test-deployment",
            "test-config"
        ]

        path_lower = path.lower()
        return any(indicator in path_lower for indicator in test_indicators)

    def check_docker_remnants(self):
        """Check for Docker test remnants"""
        print("üîç Checking Docker for test remnants...")

        try:
            # Check for test containers
            result = subprocess.run(
                ['docker', 'ps', '-a', '--filter', 'name=test', '--format', '{{.Names}}'],
                capture_output=True, text=True
            )

            if result.returncode == 0:
                containers = result.stdout.strip().split('\n')
                containers = [c for c in containers if c]

                for container in containers:
                    self.log_finding("Docker Container", container, "warning")

            # Check for test images
            result = subprocess.run(
                ['docker', 'images', '--filter', 'reference=*/test-*', '--format', '{{.Repository}}:{{.Tag}}'],
                capture_output=True, text=True
            )

            if result.returncode == 0:
                images = result.stdout.strip().split('\n')
                images = [img for img in images if img]

                for image in images:
                    self.log_finding("Docker Image", image, "info")

            # Check for test volumes
            result = subprocess.run(
                ['docker', 'volume', 'ls', '--filter', 'name=test', '--format', '{{.Name}}'],
                capture_output=True, text=True
            )

            if result.returncode == 0:
                volumes = result.stdout.strip().split('\n')
                volumes = [v for v in volumes if v]

                for volume in volumes:
                    self.log_finding("Docker Volume", volume, "warning")

        except FileNotFoundError:
            print("Docker not found - skipping Docker checks")
        except Exception as e:
            print(f"Error checking Docker remnants: {e}")

    def check_network_remnants(self):
        """Check for network test remnants"""
        print("üîç Checking network for test remnants...")

        # This would implement network-specific checks
        # Examples: test DNS entries, test network interfaces, etc.

        # Check for test DNS entries (placeholder)
        test_domains = [
            "test.example.com",
            "test-staging.example.com",
            "coolify-test.example.com"
        ]

        for domain in test_domains:
            try:
                result = subprocess.run(
                    ['nslookup', domain],
                    capture_output=True, text=True
                )

                if result.returncode == 0:
                    self.log_finding("DNS Entry", domain, "warning")

            except Exception:
                pass  # DNS lookup failed, which is expected for non-existent domains

    def check_process_remnants(self):
        """Check for running test processes"""
        print("üîç Checking processes for test remnants...")

        try:
            result = subprocess.run(
                ['ps', 'aux'],
                capture_output=True, text=True
            )

            if result.returncode == 0:
                lines = result.stdout.split('\n')

                for line in lines:
                    if 'test' in line.lower() and ('coolify' in line.lower() or 'node' in line.lower()):
                        # Extract process name
                        parts = line.split()
                        if len(parts) > 10:
                            process_name = ' '.join(parts[10:])
                            self.log_finding("Process", process_name.strip(), "info")

        except Exception as e:
            print(f"Error checking process remnants: {e}")

    def check_service_remnants(self):
        """Check for test services"""
        print("üîç Checking services for test remnants...")

        # Check systemd services
        try:
            result = subprocess.run(
                ['systemctl', 'list-units', '--all', '--no-pager'],
                capture_output=True, text=True
            )

            if result.returncode == 0:
                lines = result.stdout.split('\n')

                for line in lines:
                    if 'test' in line.lower() and ('coolify' in line.lower() or '.service' in line):
                        self.log_finding("System Service", line.strip(), "warning")

        except Exception as e:
            print(f"Error checking service remnants: {e}")

    def check_temp_files(self):
        """Check for temporary test files"""
        print("üîç Checking temporary files for test remnants...")

        temp_dirs = ['/tmp', '/var/tmp']

        for temp_dir in temp_dirs:
            if os.path.exists(temp_dir):
                try:
                    result = subprocess.run(
                        ['find', temp_dir, '-name', '*test*', '-o', '-name', '*coolify*'],
                        capture_output=True, text=True
                    )

                    if result.returncode == 0:
                        files = result.stdout.strip().split('\n')
                        files = [f for f in files if f]

                        for file in files:
                            if os.path.exists(file):
                                self.log_finding("Temp File", file, "info")

                except Exception as e:
                    print(f"Error checking temp files in {temp_dir}: {e}")

    def check_log_files(self):
        """Check for test-related log entries"""
        print("üîç Checking log files for test remnants...")

        log_files = [
            '/var/log/syslog',
            '/var/log/messages',
            '/var/log/auth.log'
        ]

        for log_file in log_files:
            if os.path.exists(log_file):
                try:
                    result = subprocess.run(
                        ['grep', '-i', 'test.*coolify\\|coolify.*test', log_file],
                        capture_output=True, text=True
                    )

                    if result.returncode == 0 and result.stdout.strip():
                        lines = result.stdout.strip().split('\n')
                        for line in lines[-5:]:  # Last 5 lines
                            self.log_finding("Log Entry", f"{log_file}: {line[:100]}...", "info")

                except Exception as e:
                    print(f"Error checking log file {log_file}: {e}")

    def generate_validation_report(self):
        """Generate validation report"""
        print("\nüìä Test Remnants Validation Report")
        print("=" * 50)

        # Count findings by severity
        critical = len([f for f in self.findings if f['severity'] == 'critical'])
        warnings = len([f for f in self.findings if f['severity'] == 'warning'])
        info = len([f for f in self.findings if f['severity'] == 'info'])

        print(f"Total findings: {len(self.findings)}")
        print(f"üî¥ Critical: {critical}")
        print(f"üü° Warnings: {warnings}")
        print(f"üü¢ Info: {info}")

        if critical > 0:
            print(f"\nüö® Critical findings require immediate attention:")
            for finding in self.findings:
                if finding['severity'] == 'critical':
                    print(f"  - {finding['category']}: {finding['item']}")

        if warnings > 0:
            print(f"\n‚ö†Ô∏è  Warnings should be reviewed:")
            for finding in self.findings:
                if finding['severity'] == 'warning':
                    print(f"  - {finding['category']}: {finding['item']}")

        # Generate detailed report file
        report_file = f"test-remnants-report-{subprocess.run(['date', '+%Y%m%d-%H%M%S'], capture_output=True, text=True).stdout.strip()}.json"

        with open(report_file, 'w') as f:
            json.dump({
                "summary": {
                    "total_findings": len(self.findings),
                    "critical": critical,
                    "warnings": warnings,
                    "info": info
                },
                "findings": self.findings
            }, f, indent=2)

        print(f"\nüìÑ Detailed report saved to: {report_file}")

        # Overall assessment
        if critical == 0 and warnings == 0:
            print("\nüéâ No test remnants found - environment is clean!")
            return True
        elif critical == 0:
            print("\n‚úÖ No critical issues found, but some cleanup may be needed")
            return True
        else:
            print("\n‚ùå Critical test remnants found - immediate cleanup required")
            return False

    def run_validation(self):
        """Run complete validation"""
        print("üîç Starting test remnants validation...")
        print("üìÖ Started at:", subprocess.run(['date'], capture_output=True, text=True).stdout.strip())
        print("")

        # Run all checks
        self.check_filesystem_remnants()
        self.check_docker_remnants()
        self.check_network_remnants()
        self.check_process_remnants()
        self.check_service_remnants()
        self.check_temp_files()
        self.check_log_files()

        # Generate report
        return self.generate_validation_report()

def main():
    validator = TestRemnantsValidator()
    success = validator.run_validation()
    exit(0 if success else 1)

if __name__ == "__main__":
    main()
```

### 2.2 Environment Integrity Validation
```bash
#!/bin/bash
# validate-environment-integrity.sh - Validate environment integrity after testing

set -euo pipefail

# Configuration
VALIDATION_LOG="integrity-validation-$(date +%Y%m%d-%H%M%S).log"
TEMP_DIR="/tmp/coolify-validation-$$"

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

# Logging
log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$VALIDATION_LOG"
}

print_status() {
    local color=$1
    local message=$2
    echo -e "${color}${message}${NC}"
}

# Create temporary directory
mkdir -p "$TEMP_DIR"

# Function to validate system resources
validate_system_resources() {
    log "Validating system resources..."

    # Check disk space
    local disk_usage=$(df / | awk 'NR==2 {print $5}' | sed 's/%//')
    if [ "$disk_usage" -lt 90 ]; then
        print_status "$GREEN" "‚úì Disk usage is acceptable: ${disk_usage}%"
    else
        print_status "$YELLOW" "‚ö†Ô∏è  High disk usage: ${disk_usage}%"
    fi

    # Check memory usage
    local mem_usage=$(free | awk 'NR==2{printf "%.0f", $3*100/$2}')
    if [ "$mem_usage" -lt 90 ]; then
        print_status "$GREEN" "‚úì Memory usage is acceptable: ${mem_usage}%"
    else
        print_status "$YELLOW" "‚ö†Ô∏è  High memory usage: ${mem_usage}%"
    fi

    # Check load average
    local load_avg=$(uptime | awk -F'load average:' '{print $2}' | awk '{print $1}' | sed 's/,//')
    if (( $(echo "$load_avg < 2.0" | bc -l) )); then
        print_status "$GREEN" "‚úì Load average is acceptable: $load_avg"
    else
        print_status "$YELLOW" "‚ö†Ô∏è  High load average: $load_avg"
    fi
}

# Function to validate network connectivity
validate_network_connectivity() {
    log "Validating network connectivity..."

    # Test basic connectivity
    if ping -c 1 8.8.8.8 &> /dev/null; then
        print_status "$GREEN" "‚úì Internet connectivity working"
    else
        print_status "$RED" "‚ùå Internet connectivity failed"
    fi

    # Test DNS resolution
    if nslookup google.com &> /dev/null; then
        print_status "$GREEN" "‚úì DNS resolution working"
    else
        print_status "$RED" "‚ùå DNS resolution failed"
    fi

    # Test local ports (if Coolify should be running)
    local coolify_ports=(80 443 3000)
    for port in "${coolify_ports[@]}"; do
        if netstat -tuln | grep -q ":$port "; then
            print_status "$GREEN" "‚úì Port $port is listening"
        else
            print_status "$BLUE" "‚ÑπÔ∏è  Port $port is not listening (may be normal)"
        fi
    done
}

# Function to validate file permissions
validate_file_permissions() {
    log "Validating file permissions..."

    # Check critical directories
    local critical_dirs=("/etc" "/var/log" "/home" "/tmp")

    for dir in "${critical_dirs[@]}"; do
        if [ -d "$dir" ]; then
            local perms=$(stat -c "%a" "$dir")
            case "$dir" in
                "/etc")
                    if [ "$perms" = "755" ]; then
                        print_status "$GREEN" "‚úì $dir permissions correct: $perms"
                    else
                        print_status "$YELLOW" "‚ö†Ô∏è  $dir permissions unusual: $perms"
                    fi
                    ;;
                "/tmp")
                    if [ "$perms" = "1777" ] || [ "$perms" = "777" ]; then
                        print_status "$GREEN" "‚úì $dir permissions correct: $perms"
                    else
                        print_status "$YELLOW" "‚ö†Ô∏è  $dir permissions unusual: $perms"
                    fi
                    ;;
                *)
                    if [ "$perms" = "755" ] || [ "$perms" = "750" ]; then
                        print_status "$GREEN" "‚úì $dir permissions acceptable: $perms"
                    else
                        print_status "$YELLOW" "‚ö†Ô∏è  $dir permissions unusual: $perms"
                    fi
                    ;;
            esac
        fi
    done
}

# Function to validate service configuration
validate_service_configuration() {
    log "Validating service configuration..."

    # Check if required services are running
    local required_services=("docker" "nginx")

    for service in "${required_services[@]}"; do
        if systemctl is-active --quiet "$service"; then
            print_status "$GREEN" "‚úì Service $service is running"
        elif systemctl list-unit-files | grep -q "^$service.service"; then
            print_status "$YELLOW" "‚ö†Ô∏è  Service $service is installed but not running"
        else
            print_status "$BLUE" "‚ÑπÔ∏è  Service $service is not installed"
        fi
    done
}

# Function to validate security settings
validate_security_settings() {
    log "Validating security settings..."

    # Check firewall status
    if command -v ufw &> /dev/null; then
        if ufw status | grep -q "Status: active"; then
            print_status "$GREEN" "‚úì UFW firewall is active"
        else
            print_status "$YELLOW" "‚ö†Ô∏è  UFW firewall is not active"
        fi
    fi

    # Check for open SSH ports
    if netstat -tuln | grep -q ":22 "; then
        print_status "$BLUE" "‚ÑπÔ∏è  SSH port 22 is open"
    fi

    # Check for suspicious processes
    local suspicious_procs=$(ps aux | grep -E "(nc -l|ncat -l|python.*-m http.server)" | grep -v grep || true)
    if [ -n "$suspicious_procs" ]; then
        print_status "$YELLOW" "‚ö†Ô∏è  Potentially suspicious processes found:"
        echo "$suspicious_procs"
    else
        print_status "$GREEN" "‚úì No suspicious processes detected"
    fi
}

# Function to validate Docker environment
validate_docker_environment() {
    log "Validating Docker environment..."

    if command -v docker &> /dev/null; then
        # Check if Docker daemon is running
        if docker info &> /dev/null; then
            print_status "$GREEN" "‚úì Docker daemon is running"

            # Check for dangling images
            local dangling_images=$(docker images -f "dangling=true" -q | wc -l)
            if [ "$dangling_images" -eq 0 ]; then
                print_status "$GREEN" "‚úì No dangling Docker images"
            else
                print_status "$YELLOW" "‚ö†Ô∏è  Found $dangling_images dangling Docker images"
            fi

            # Check for unused volumes
            local unused_volumes=$(docker volume ls -f "dangling=true" -q | wc -l)
            if [ "$unused_volumes" -eq 0 ]; then
                print_status "$GREEN" "‚úì No unused Docker volumes"
            else
                print_status "$YELLOW" "‚ö†Ô∏è  Found $unused_volumes unused Docker volumes"
            fi

        else
            print_status "$RED" "‚ùå Docker daemon is not running"
        fi
    else
        print_status "$BLUE" "‚ÑπÔ∏è  Docker is not installed"
    fi
}

# Function to generate validation report
generate_validation_report() {
    log "Generating validation report..."

    local report_file="integrity-validation-report-$(date +%Y%m%d-%H%M%S).json"

    cat > "$report_file" << EOF
{
  "validation_summary": {
    "timestamp": "$(date -Iseconds)",
    "log_file": "$VALIDATION_LOG",
    "temp_dir": "$TEMP_DIR"
  },
  "checks_performed": [
    "System resources validation",
    "Network connectivity validation",
    "File permissions validation",
    "Service configuration validation",
    "Security settings validation",
    "Docker environment validation"
  ],
  "overall_status": "completed"
}
EOF

    print_status "$GREEN" "‚úì Validation report generated: $report_file"
    log "Validation report generated: $report_file"
}

# Main validation function
main() {
    print_status "$BLUE" "üîç Starting environment integrity validation..."
    print_status "$BLUE" "Log file: $VALIDATION_LOG"
    print_status "$BLUE" "Temp dir: $TEMP_DIR"
    echo ""

    # Run validation checks
    validate_system_resources
    validate_network_connectivity
    validate_file_permissions
    validate_service_configuration
    validate_security_settings
    validate_docker_environment

    echo ""
    print_status "$BLUE" "üìä Generating validation report..."
    generate_validation_report

    print_status "$GREEN" "üéâ Environment integrity validation completed!"
    log "Environment integrity validation completed successfully"
}

# Cleanup function
cleanup() {
    if [ -d "$TEMP_DIR" ]; then
        rm -rf "$TEMP_DIR"
        log "Cleaned up temporary directory: $TEMP_DIR"
    fi
}

# Set trap for cleanup
trap cleanup EXIT

# Run main function
main
```

## 3. Backup and Restore Mechanisms

### 3.1 Automated Backup Script
```python
#!/usr/bin/env python3
# automated-backup.py - Automated backup system for Coolify testing

import os
import json
import subprocess
import shutil
import tarfile
from datetime import datetime, timedelta
from pathlib import Path

class AutomatedBackup:
    def __init__(self, config_file="backup-config.json"):
        self.config = self.load_config(config_file)
        self.backup_dir = Path(f"backups/{datetime.now().strftime('%Y%m%d-%H%M%S')}")
        self.backup_dir.mkdir(parents=True, exist_ok=True)

    def load_config(self, config_file):
        """Load backup configuration"""
        default_config = {
            "backup_types": {
                "config_files": True,
                "docker_data": True,
                "databases": True,
                "application_data": True,
                "logs": True
            },
            "backup_paths": {
                "config": [
                    ".coolify/",
                    "docker-compose.yml",
                    "environment.yml",
                    "*.conf"
                ],
                "data": [
                    "data/",
                    "uploads/",
                    "user-data/"
                ],
                "logs": [
                    "logs/",
                    "/var/log/coolify*"
                ]
            },
            "retention_days": 30,
            "compression": True,
            "encryption": False,
            "remote_backup": {
                "enabled": False,
                "type": "s3",  # s3, gcs, azure
                "config": {}
            }
        }

        try:
            with open(config_file, 'r') as f:
                config = json.load(f)
                return {**default_config, **config}
        except FileNotFoundError:
            return default_config

    def log(self, message):
        """Log message"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_message = f"[{timestamp}] {message}"
        print(log_message)

        # Also log to backup directory
        log_file = self.backup_dir / "backup.log"
        with open(log_file, 'a') as f:
            f.write(log_message + '\n')

    def backup_config_files(self):
        """Backup configuration files"""
        if not self.config['backup_types']['config_files']:
            return True

        self.log("Backing up configuration files...")

        config_backup_dir = self.backup_dir / "config"
        config_backup_dir.mkdir(exist_ok=True)

        try:
            for path_pattern in self.config['backup_paths']['config']:
                # Expand pattern
                paths = Path('.').glob(path_pattern)

                for path in paths:
                    if path.is_file():
                        dest = config_backup_dir / path.name
                        shutil.copy2(path, dest)
                        self.log(f"Backed up config file: {path}")
                    elif path.is_dir():
                        dest = config_backup_dir / path.name
                        shutil.copytree(path, dest, ignore=shutil.ignore_patterns('.git', 'node_modules'))
                        self.log(f"Backed up config directory: {path}")

            return True
        except Exception as e:
            self.log(f"Config backup failed: {e}")
            return False

    def backup_docker_data(self):
        """Backup Docker containers and images"""
        if not self.config['backup_types']['docker_data']:
            return True

        self.log("Backing up Docker data...")

        docker_backup_dir = self.backup_dir / "docker"
        docker_backup_dir.mkdir(exist_ok=True)

        try:
            # Backup container configurations
            result = subprocess.run(
                ['docker', 'ps', '-a', '--format', '{{.Names}}'],
                capture_output=True, text=True
            )

            if result.returncode == 0:
                containers = result.stdout.strip().split('\n')
                containers = [c for c in containers if c]

                for container in containers:
                    try:
                        result = subprocess.run(
                            ['docker', 'inspect', container],
                            capture_output=True, text=True
                        )
                        if result.returncode == 0:
                            with open(docker_backup_dir / f"{container}.json", 'w') as f:
                                f.write(result.stdout)
                            self.log(f"Backed up container config: {container}")
                    except Exception as e:
                        self.log(f"Failed to backup container {container}: {e}")

                # Export containers
                export_file = docker_backup_dir / "containers.tar"
                if containers:
                    try:
                        subprocess.run(['docker', 'export'] + containers + ['-o', str(export_file)], check=True)
                        self.log(f"Exported containers to: {export_file}")
                    except subprocess.CalledProcessError as e:
                        self.log(f"Container export failed: {e}")

            # Backup images list
            try:
                result = subprocess.run(
                    ['docker', 'images', '--format', '{{.Repository}}:{{.Tag}}'],
                    capture_output=True, text=True
                )
                if result.returncode == 0:
                    with open(docker_backup_dir / "images.txt", 'w') as f:
                        f.write(result.stdout)
                    self.log("Backed up Docker images list")
            except Exception as e:
                self.log(f"Failed to backup images list: {e}")

            # Backup volumes
            try:
                result = subprocess.run(
                    ['docker', 'volume', 'ls', '--format', '{{.Name}}'],
                    capture_output=True, text=True
                )
                if result.returncode == 0:
                    volumes = result.stdout.strip().split('\n')
                    volumes = [v for v in volumes if v]

                    for volume in volumes:
                        # Backup volume data
                        volume_backup_dir = docker_backup_dir / "volumes" / volume
                        volume_backup_dir.mkdir(parents=True, exist_ok=True)

                        try:
                            # Copy volume data
                            subprocess.run([
                                'docker', 'run', '--rm',
                                '-v', f'{volume}:/source',
                                '-v', f'{volume_backup_dir}:/backup',
                                'alpine',
                                'cp', '-r', '/source/.', '/backup/'
                            ], check=True)
                            self.log(f"Backed up volume: {volume}")
                        except subprocess.CalledProcessError as e:
                            self.log(f"Failed to backup volume {volume}: {e}")
            except Exception as e:
                self.log(f"Volume backup failed: {e}")

            return True
        except Exception as e:
            self.log(f"Docker backup failed: {e}")
            return False

    def backup_databases(self):
        """Backup database data"""
        if not self.config['backup_types']['databases']:
            return True

        self.log("Backing up databases...")

        db_backup_dir = self.backup_dir / "databases"
        db_backup_dir.mkdir(exist_ok=True)

        try:
            # This would implement database-specific backup logic
            # Examples: PostgreSQL, MySQL, MongoDB, etc.

            # PostgreSQL backup example
            if shutil.which('pg_dump'):
                try:
                    backup_file = db_backup_dir / "postgresql_backup.sql"
                    subprocess.run([
                        'pg_dump', '-h', 'localhost', '-U', 'postgres', '-d', 'coolify_db'
                    ], stdout=open(backup_file, 'w'), check=True)
                    self.log("Backed up PostgreSQL database")
                except subprocess.CalledProcessError as e:
                    self.log(f"PostgreSQL backup failed: {e}")

            # MySQL backup example
            if shutil.which('mysqldump'):
                try:
                    backup_file = db_backup_dir / "mysql_backup.sql"
                    subprocess.run([
                        'mysqldump', '-h', 'localhost', '-u', 'root', 'coolify_db'
                    ], stdout=open(backup_file, 'w'), check=True)
                    self.log("Backed up MySQL database")
                except subprocess.CalledProcessError as e:
                    self.log(f"MySQL backup failed: {e}")

            return True
        except Exception as e:
            self.log(f"Database backup failed: {e}")
            return False

    def backup_application_data(self):
        """Backup application data"""
        if not self.config['backup_types']['application_data']:
            return True

        self.log("Backing up application data...")

        data_backup_dir = self.backup_dir / "data"
        data_backup_dir.mkdir(exist_ok=True)

        try:
            for path_pattern in self.config['backup_paths']['data']:
                paths = Path('.').glob(path_pattern)

                for path in paths:
                    if path.exists():
                        dest = data_backup_dir / path.name
                        if path.is_file():
                            shutil.copy2(path, dest)
                        else:
                            shutil.copytree(path, dest, ignore=shutil.ignore_patterns('.git', 'node_modules', 'tmp'))
                        self.log(f"Backed up application data: {path}")

            return True
        except Exception as e:
            self.log(f"Application data backup failed: {e}")
            return False

    def backup_logs(self):
        """Backup log files"""
        if not self.config['backup_types']['logs']:
            return True

        self.log("Backing up log files...")

        logs_backup_dir = self.backup_dir / "logs"
        logs_backup_dir.mkdir(exist_ok=True)

        try:
            for path_pattern in self.config['backup_paths']['logs']:
                # Handle both relative and absolute paths
                if path_pattern.startswith('/'):
                    paths = Path(path_pattern).parent.glob(Path(path_pattern).name)
                else:
                    paths = Path('.').glob(path_pattern)

                for path in paths:
                    if path.is_file():
                        dest = logs_backup_dir / path.name
                        shutil.copy2(path, dest)
                        self.log(f"Backed up log file: {path}")
                    elif path.is_dir():
                        dest = logs_backup_dir / path.name
                        shutil.copytree(path, dest, ignore=shutil.ignore_patterns('*.tmp'))
                        self.log(f"Backed up log directory: {path}")

            return True
        except Exception as e:
            self.log(f"Log backup failed: {e}")
            return False

    def compress_backup(self):
        """Compress backup if enabled"""
        if not self.config.get('compression', True):
            return True

        self.log("Compressing backup...")

        try:
            compressed_file = f"{self.backup_dir}.tar.gz"

            with tarfile.open(compressed_file, 'w:gz') as tar:
                tar.add(self.backup_dir, arcname=self.backup_dir.name)

            # Remove uncompressed backup
            shutil.rmtree(self.backup_dir)
            self.backup_dir = Path(compressed_file)

            self.log(f"Backup compressed: {compressed_file}")
            return True
        except Exception as e:
            self.log(f"Backup compression failed: {e}")
            return False

    def cleanup_old_backups(self):
        """Clean up old backups based on retention policy"""
        retention_days = self.config.get('retention_days', 30)
        cutoff_date = datetime.now() - timedelta(days=retention_days)

        self.log(f"Cleaning up backups older than {retention_days} days...")

        backups_root = Path("backups")
        if not backups_root.exists():
            return True

        try:
            deleted_count = 0
            for backup_path in backups_root.iterdir():
                if backup_path.is_dir() or backup_path.suffix in ['.tar', '.tar.gz']:
                    # Extract date from backup name
                    try:
                        date_str = backup_path.name.split('-')[0]  # Assuming format YYYYMMDD-...
                        backup_date = datetime.strptime(date_str, '%Y%m%d')

                        if backup_date < cutoff_date:
                            if backup_path.is_dir():
                                shutil.rmtree(backup_path)
                            else:
                                backup_path.unlink()
                            deleted_count += 1
                            self.log(f"Deleted old backup: {backup_path}")
                    except ValueError:
                        # Skip if date can't be parsed
                        pass

            self.log(f"Cleaned up {deleted_count} old backups")
            return True
        except Exception as e:
            self.log(f"Backup cleanup failed: {e}")
            return False

    def create_backup_manifest(self):
        """Create backup manifest file"""
        self.log("Creating backup manifest...")

        manifest = {
            "backup_info": {
                "created_at": datetime.now().isoformat(),
                "backup_path": str(self.backup_dir),
                "hostname": subprocess.run(['hostname'], capture_output=True, text=True).stdout.strip(),
                "user": subprocess.run(['whoami'], capture_output=True, text=True).stdout.strip()
            },
            "config": self.config,
            "components": []
        }

        # Add component information
        backup_path = self.backup_dir
        if self.backup_dir.suffix in ['.tar', '.tar.gz']:
            # For compressed backups, we'd need to extract to inspect
            backup_path = self.backup_dir.with_suffix('')

        if backup_path.exists():
            for component in backup_path.iterdir():
                if component.is_dir():
                    component_info = {
                        "name": component.name,
                        "type": "directory",
                        "size_bytes": sum(f.stat().st_size for f in component.rglob('*') if f.is_file())
                    }
                else:
                    component_info = {
                        "name": component.name,
                        "type": "file",
                        "size_bytes": component.stat().st_size
                    }
                manifest["components"].append(component_info)

        manifest_file = f"{self.backup_dir}.manifest.json"
        with open(manifest_file, 'w') as f:
            json.dump(manifest, f, indent=2)

        self.log(f"Backup manifest created: {manifest_file}")

    def run_backup(self):
        """Run complete backup process"""
        self.log("üîÑ Starting automated backup process...")
        self.log(f"Backup directory: {self.backup_dir}")

        success = True

        # Run backup components
        success &= self.backup_config_files()
        success &= self.backup_docker_data()
        success &= self.backup_databases()
        success &= self.backup_application_data()
        success &= self.backup_logs()

        if success:
            # Compress backup
            self.compress_backup()

            # Create manifest
            self.create_backup_manifest()

            # Clean up old backups
            self.cleanup_old_backups()

            self.log("üéâ Backup process completed successfully!")
            return True
        else:
            self.log("‚ùå Backup process completed with errors")
            return False

def main():
    import argparse

    parser = argparse.ArgumentParser(description='Automated backup system')
    parser.add_argument('--config', default='backup-config.json', help='Configuration file')
    parser.add_argument('--dry-run', action='store_true', help='Show what would be backed up')

    args = parser.parse_args()

    backup = AutomatedBackup(args.config)

    if args.dry_run:
        print("DRY RUN MODE - Configuration:")
        print(json.dumps(backup.config, indent=2))
        return

    success = backup.run_backup()
    exit(0 if success else 1)

if __name__ == "__main__":
    main()
```

## 4. Usage Examples

### 4.1 Basic Cleanup Operations
```bash
# Dry run to see what would be cleaned up
./cleanup-test-resources.sh --dry-run

# Perform actual cleanup with backups
./cleanup-test-resources.sh --test-prefix mytest-

# Cleanup without backups (fast but irreversible)
./cleanup-test-resources.sh --no-backup

# Environment reset
python3 environment-reset.py --config reset-config.json

# Validate no remnants remain
python3 validate-test-remnants.py
```

### 4.2 Backup Operations
```bash
# Create automated backup
python3 automated-backup.py

# Custom backup configuration
python3 automated-backup.py --config my-backup-config.json

# Dry run backup
python3 automated-backup.py --dry-run
```

### 4.3 Validation Operations
```bash
# Complete environment validation
./validate-environment-integrity.sh

# Check for test remnants
python3 validate-test-remnants.py

# Validate specific components
python3 validate-test-remnants.py --category docker
```

## 5. Configuration Files

### 5.1 Cleanup Configuration
```json
{
  "backup_before_delete": true,
  "test_prefix": "test-",
  "preserve_resources": {
    "databases": ["prod_db"],
    "volumes": ["important_data"],
    "config_files": [".env"]
  },
  "cleanup_scope": {
    "projects": true,
    "applications": true,
    "databases": true,
    "domains": true,
    "docker_resources": true,
    "local_files": true
  }
}
```

### 5.2 Backup Configuration
```json
{
  "backup_types": {
    "config_files": true,
    "docker_data": true,
    "databases": true,
    "application_data": true,
    "logs": true
  },
  "backup_paths": {
    "config": [
      ".coolify/",
      "docker-compose.yml",
      "*.conf"
    ],
    "data": [
      "data/",
      "uploads/"
    ],
    "logs": [
      "logs/",
      "/var/log/coolify*"
    ]
  },
  "retention_days": 30,
  "compression": true,
  "encryption": false
}
```

## Conclusion

This comprehensive cleanup and validation framework ensures that testing processes maintain system integrity, provide proper backup mechanisms, and validate complete removal of test artifacts. The procedures are designed to be safe, reversible, and thorough while providing detailed logging and reporting capabilities.